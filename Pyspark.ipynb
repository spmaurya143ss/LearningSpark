{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVWn4KfmrIe/OuQbzn+Yzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spmaurya143ss/LearningSpark/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sqaBYBM9Hkq",
        "outputId": "8a2929c9-4ee8-4c3f-c913-b200c67bec19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.session import SparkSession"
      ],
      "metadata": {
        "id": "LcIjjrzH9W7g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark =  SparkSession.builder.appName(\"DisplayDataFrame\").getOrCreate()"
      ],
      "metadata": {
        "id": "Gj5-PRLO9qUH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('Shivam',25,'M'),('Hritik',24,'M'),('Shweta',24,'F')]\n",
        "df = spark.createDataFrame(data, ['Name', 'Age','Gender'])"
      ],
      "metadata": {
        "id": "13YMAR0n-Xxj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvFCDKtj-yUq",
        "outputId": "03e26a36-c13a-4707-fd88-a6ae993a2b08"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+------+\n",
            "|  Name|Age|Gender|\n",
            "+------+---+------+\n",
            "|Shivam| 25|     M|\n",
            "|Hritik| 24|     M|\n",
            "|Shweta| 24|     F|\n",
            "+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myRange = spark.range(1000).toDF('Number')\n",
        "# myRange.show()"
      ],
      "metadata": {
        "id": "3CFGBzVk_b6s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divBy2 = myRange.where(\"Number % 2 = 0\")\n",
        "# divBy2.show()"
      ],
      "metadata": {
        "id": "ecWapixF_261"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divBy2.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJT9nc8_AUay",
        "outputId": "19c0ca9a-2d45-4bc6-800c-4d206fe34460"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/databricks/Spark-The-Definitive-Guide"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT74qIhOBAG8",
        "outputId": "47cb105e-739f-461e-84ca-991fd7a8563b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spark-The-Definitive-Guide'...\n",
            "remote: Enumerating objects: 2035, done.\u001b[K\n",
            "remote: Total 2035 (delta 0), reused 0 (delta 0), pack-reused 2035 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2035/2035), 523.88 MiB | 20.47 MiB/s, done.\n",
            "Resolving deltas: 100% (305/305), done.\n",
            "Updating files: 100% (1738/1738), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Spark-The-Definitive-Guide/data/flight-data/csv/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x3WZVhyDvZG",
        "outputId": "dbfbc789-db82-4582-f9b6-ebdc7969cfbf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Spark-The-Definitive-Guide/data/flight-data/csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"Spark-The-Definitive-Guide/data/flight-data/csv\")\n",
        "# flightData2015.show()"
      ],
      "metadata": {
        "id": "MZYqWOeWEFpp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DbuoH5FFjIn",
        "outputId": "1ee49054-2563-48f5-cf45-36f39411b5ef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=69)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.sort(\"count\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kprolsIJRgz",
        "outputId": "468d4378-bb2f-4c6e-965c-d07ccab4bea0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [count#5790 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(count#5790 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=241]\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#5788,ORIGIN_COUNTRY_NAME#5789,count#5790] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/Spark-The-Definitive-Guide/data/flight-data/csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
      ],
      "metadata": {
        "id": "gtjvGcWSKr_1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GR3Ls66K45T",
        "outputId": "85457ba1-0943-4c44-8385-3b457ccc23ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.createOrReplaceTempView(\"Flight_Data_2015\")"
      ],
      "metadata": {
        "id": "TKF3_JSFOrx7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWay = spark.sql(\"select DEST_COUNTRY_NAME, count(*) as Count from Flight_Data_2015 group by DEST_COUNTRY_NAME\")#.where(\"Count = 2\")\n",
        "# sqlWay.show()"
      ],
      "metadata": {
        "id": "7pTbdcbpO6P2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWay.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzSUL2RrR1uo",
        "outputId": "5a6b8767-fbd1-4291-9b6c-9f4825178608"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#5788, 200), ENSURE_REQUIREMENTS, [plan_id=263]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#5788] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/Spark-The-Definitive-Guide/data/flight-data/csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
        "# dataFrameWay.show()"
      ],
      "metadata": {
        "id": "ypw7SwobTTX5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrameWay.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnE1oV50TqP2",
        "outputId": "a70bc3d6-c4c2-4ead-9bac-f115d94a5cd8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#5788, 200), ENSURE_REQUIREMENTS, [plan_id=276]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#5788] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/Spark-The-Definitive-Guide/data/flight-data/csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWayMax = spark.sql(\"select max(count) from Flight_Data_2015\")\n",
        "# sqlWayMax.show()"
      ],
      "metadata": {
        "id": "ry_qSMxyWNQJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max"
      ],
      "metadata": {
        "id": "T-0eSPXFWtrs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.select(max(\"count\")).take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-SGJzOWYN5K",
        "outputId": "344dc9cd-58df-4cf0-8cf4-2b1da98ab695"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxSql = spark.sql(\"select DEST_COUNTRY_NAME, sum(count) as destination_total from Flight_Data_2015 group by DEST_COUNTRY_NAME order by sum(count) DESC limit 5\")\n",
        "maxSql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nf4alu_Y0pB",
        "outputId": "ef33a12a-46fe-4f95-e540-0da1682958fb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|          2348280|\n",
            "|           Canada|            49052|\n",
            "|           Mexico|            38075|\n",
            "|   United Kingdom|            10946|\n",
            "|            Japan|             9205|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc"
      ],
      "metadata": {
        "id": "ntKQNz4Hb36T"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max5= flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).limit(5)\n",
        "# max5.show()"
      ],
      "metadata": {
        "id": "yA3SxqOmbyDn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max5.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UYybbTRfuBa",
        "outputId": "4590a306-2287-4643-b46e-9c739a94b085"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#5854L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#5788,destination_total#5854L])\n",
            "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[sum(count#5790)])\n",
            "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#5788, 200), ENSURE_REQUIREMENTS, [plan_id=369]\n",
            "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#5788], functions=[partial_sum(count#5790)])\n",
            "            +- FileScan csv [DEST_COUNTRY_NAME#5788,count#5790] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/Spark-The-Definitive-Guide/data/flight-data/csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "staticDataFrame = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\",'true').load(\"Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")"
      ],
      "metadata": {
        "id": "dp_8KKJVK9xz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "staticDataFrame.createOrReplaceTempView(\"retail_data\")"
      ],
      "metadata": {
        "id": "v7yQw6QVLnMf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "staticSchema = staticDataFrame.schema"
      ],
      "metadata": {
        "id": "2OwtohDjLw3f"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import window, column, desc, col\n"
      ],
      "metadata": {
        "id": "zH7upQf7NXtD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oneDayDF = staticDataFrame.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\").groupby( col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\")\n",
        "# oneDayDF.show()"
      ],
      "metadata": {
        "id": "DyhILXiFSuN7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\", 1).format(\"csv\").option(\"header\", \"true\").load(\"Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")"
      ],
      "metadata": {
        "id": "1zNUfCHuXIwc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame.isStreaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc3JG3GPbcVt",
        "outputId": "92fe234e-47d6-42d5-8de0-c8bce44f9221"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "purchasedByCustomerPerHour = streamingDataFrame.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\").groupby( col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\")"
      ],
      "metadata": {
        "id": "MOCHnnWRcJSW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purchasedByCustomerPerHour.writeStream.format(\"console\").queryName(\"customer_purchases2\").outputMode(\"complete\").start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuH2nH7Jc3-u",
        "outputId": "d37f6515-7363-4367-9d94-325242182e10"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7a50103394e0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "staticDataFrame.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl8Q1GZCdI08",
        "outputId": "4e1b3eef-31e6-4e1f-b17a-50c3ea5ddaea"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, col\n",
        "preppedDataFrame = staticDataFrame.na.fill(0).withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\")).coalesce(5)\n",
        "# preppedDataFrame.show()"
      ],
      "metadata": {
        "id": "ZgI0KbcyhzQd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
        "testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")"
      ],
      "metadata": {
        "id": "snuJpuC9kNkD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataFrame.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzAVu-bqkSwp",
        "outputId": "94e136bf-0d2c-4c70-b48d-26d2a4710086"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "245903"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testDataFrame.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClGqrfLQkmg5",
        "outputId": "4ee55e55-eb22-4d3f-934b-f70fad4fb82c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296006"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_index\")\n",
        "# indexer.show()"
      ],
      "metadata": {
        "id": "frcAgdaulZRr"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "encoder = OneHotEncoder().setInputCol(\"day_of_week_index\").setOutputCol(\"day_of_week_encoded\")"
      ],
      "metadata": {
        "id": "G8eOEEwWltV_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectAssembler = VectorAssembler().setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"]).setOutputCol(\"features\")"
      ],
      "metadata": {
        "id": "lmcn0q5tmcaZ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "transformationPipeline = Pipeline().setStages([indexer, encoder,vectAssembler])"
      ],
      "metadata": {
        "id": "znpM3vQNnC9r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
      ],
      "metadata": {
        "id": "qSs0I1rzo4m2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformedTraining = fittedPipeline.transform(trainDataFrame)"
      ],
      "metadata": {
        "id": "im3FtIXqpCU7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformedTraining.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EaEDpYzsre-",
        "outputId": "f80ca4f3-4646-48cd-8b6a-7164dbaca3c0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "kmeans = KMeans().setK(20).setSeed(11)"
      ],
      "metadata": {
        "id": "H_8gMLMptxMA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmModel = kmeans.fit(transformedTraining)"
      ],
      "metadata": {
        "id": "5SKxq4p1x6ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmModel.computeCost(transformedTraining)"
      ],
      "metadata": {
        "id": "KWNWODLM1Yjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}